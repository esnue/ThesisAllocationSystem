{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esnue/ThesisAllocationSystem/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzDCH_RhlHCt"
      },
      "source": [
        "## **Model**\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This code script details the steps taken to develop a Transformers- and Pre-Trained based Language multi-label Text-Classification Model. \n",
        "\n",
        "The goal of the model is the ability to match students based on their thesis proposals to supervisors based on the content of their academic papers.\n",
        "\n",
        "Using academic papers as train data is an approach that was proposed by the similarly-working [Toronto Matching System](https://www.cs.toronto.edu/~zemel/documents/tpms.pdf). Instead of matching students to professors, the Toronto Matching Systems assigns Peer-Reviewers based on their academic papers to submitted papers.\n",
        "\n",
        "While thesis proposals tend to be coherent in their structure, academic papers usually have very differing structures, depending on the layout that the publisher demands. Constraining ourselves to few or one particular Journal outlet could have made the cleaning process and possibly the model training easier but would also have considerably reduced the size of the train data. \n",
        "\n",
        "Our multi-label text-classification method is based on Transformers and Pre-Trained Language Models. The Pre-Trained Mode is state-of-art [DistilBert](https://arxiv.org/abs/1910.01108). The Transformers version is [4.4.2](https://huggingface.co/transformers/).\n",
        "\n",
        "We are finetuning a pretrained DistilBERT model for multilabel text classification. This is a very common application of text classification, where a given document can be classified into one or more categories. This approach best mirrors our use case where a thesis proposal could likely be allocated to more than one research area, given the interdisciplinary nature and overlap of research areas between chairs.\n",
        "\n",
        "Before you attempt to run the script, make sure to secure the required modules and datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Hc875clTDX"
      },
      "source": [
        "## **Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e3GrVM9lDTy"
      },
      "source": [
        "! pip install transformers==3.0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JhDfyvQm13A"
      },
      "source": [
        "# Import requirements\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWdtr0Jmm50S"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_wEr72pnD1y"
      },
      "source": [
        "<a id='section02'></a>\n",
        "### Importing and Pre-Processing the domain data\n",
        "\n",
        "The required data set to run this script is `train-papers-label.csv`. It can be downloaded [here](https://drive.google.com/file/d/1-12x2qro_m9HqWUEwZU_l4njMJ6y1LoX/view?usp=sharing). Please make sure you've stored it on your GDrive or on your computer.\n",
        "\n",
        "The section of the script implements the following tasks:\n",
        "\n",
        "* Load the dataset `train-papers-label.csv`\n",
        "* The dataset is prepared for the DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq-DtMlBnyiU",
        "outputId": "1a8bcaaa-9b6e-487f-802a-46a4e0600642"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8LKOlxVoO1c"
      },
      "source": [
        "Make sure that list items are correctly stored and read as integers, not as strings, which can happen while saving a data frame as csv. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bewac5D9oL6u",
        "outputId": "eb692cab-2127-44eb-94b4-4a27ced0dee5"
      },
      "source": [
        "# Load df\n",
        "new_df = pd.read_csv('/content/drive/MyDrive/ThesisAllocationSystem/data_final/train-papers-label.csv', converters={'labels': eval})\n",
        "\n",
        "# Rename df col\n",
        "new_df.rename(columns={\"content\": \"text\"}, inplace = True)\n",
        "\n",
        "# Check content & type\n",
        "print(new_df.sample(10))\n",
        "print(type(new_df['labels'].iloc[1]))\n",
        "print(type(new_df['labels'].iloc[1][1]))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  text                                             labels\n",
            "360  b\"Economic uncertainty and fertility postponem...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "42   b\"NimbusRomNo9L-Regu\\n\\n\\nThis is an Open Acce...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "316  b'SAQ114_1_14Wagner_Fpp.indd\\n\\n\\nA G A I N S ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
            "234  b'Leveraging Regional Differences and Crossbor...  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "697  b'untitled\\n\\n\\nFull Terms & Conditions of acc...  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
            "79   b'Evolutionary Psychology and Artificial\\n\\nIn...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "421  b\"A Predictive Test of Voters' Economic Benchm...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "610  b'Electronic copy available at: http://ssrn.co...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "350  b'Demographic Research   a free, expedited, on...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "451  b'Tony Blairs gamble: The Middle East Peace Pr...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
            "<class 'list'>\n",
            "<class 'int'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYFM-i6Ko61S",
        "outputId": "9e713604-533a-4188-d2ec-8f06d7166a66"
      },
      "source": [
        "# Check number of labels\n",
        "print(len(new_df['labels'].iloc[1]))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gebxzUbBpS3W"
      },
      "source": [
        "<a id='section03'></a>\n",
        "### Preparing the Dataset and Dataloader\n",
        "\n",
        "First, we define some key variables used for training/fine tuning later on. Next, we create a MultiLabelDataset class alongside a DataLoader specifying how the text is pre-processed prior to sending it to the neural network and the number of batches to be sent to the neural network for training.\n",
        "\n",
        "Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "#### *MultiLabelDataset* Dataset Class\n",
        "- This class is defined to accept the `tokenizer`, `dataframe` and `max_length` as input and generate tokenized output and tags that is used by the BERT model for training. \n",
        "- We are using the DistilBERT tokenizer to tokenize the data in the `text` column of the dataframe.\n",
        "- The tokenizer uses the `encode_plus` method to perform tokenization and generate the necessary outputs, namely: `ids`, `attention_mask`, `token_type_ids`\n",
        "\n",
        "- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer)\n",
        "- `targets` is the list of categories labled as `0` or `1` in the dataframe. \n",
        "- The *MultiLabelDataset* class is used to create 2 datasets, for training and for validation.\n",
        "- *Training Dataset* is used to fine tune the model: **currently 80% of the original data, but at later stage we need to include test data from different source**\n",
        "- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training. \n",
        "\n",
        "#### Dataloader\n",
        "- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
        "- This control is achieved using the parameters such as `batch_size` and `max_len`.\n",
        "- Training and Validation dataloaders are used in the training and validation part of the flow respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8QW54O8pe2H"
      },
      "source": [
        "# Configurations\n",
        "\n",
        "# Defining key var\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "# Import DistilBert Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC7zA4nYDGX3"
      },
      "source": [
        "class MultiLabelDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.labels\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOZWsLfGDOBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4155985c-3ddb-4022-becd-bb3217ebe280"
      },
      "source": [
        "# Creating the train dataset and dataloader for the neural network\n",
        "train_size = 0.8\n",
        "train_data=new_df.sample(frac=train_size,random_state=200)\n",
        "test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "print(type(train_data['labels'].iloc[1][1]))\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
        "\n",
        "training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
        "testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'int'>\n",
            "FULL Dataset: (811, 2)\n",
            "TRAIN Dataset: (649, 2)\n",
            "TEST Dataset: (162, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICkvMpj3p4zW"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew_dXHaZKvah"
      },
      "source": [
        "# next(iter(training_loader))\n",
        "# next(iter(testing_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6_YPU38p_wU"
      },
      "source": [
        "<a id='section04'></a>\n",
        "### Creating the Neural Network for Fine Tuning\n",
        "\n",
        "#### Neural Network\n",
        " - We will be creating a neural network with the `DistilBERTClass`. \n",
        " - This network will have the `DistilBERT` model.  Follwed by a `Droput` and `Linear Layer`. They are added for the purpose of **Regularization** and **Classification** respectively. \n",
        " - In the forward loop, there are 2 output from the `DistilBERTClass` layer.\n",
        " - The second output `output_1` or called the `pooled output` is passed to the `Drop Out layer` and the subsequent output is given to the `Linear layer`. \n",
        " - Keep note the number of dimensions for `Linear Layer` is **30** because that is the total number of categories in which we are looking to classify our model\n",
        " - The data will be fed to the `DistilBERTClass` as defined in the dataset. \n",
        " - Final layer outputs is what will be used to calcuate the loss and to determine the accuracy of models prediction. \n",
        " - We will initiate an instance of the network called `model`. This instance will be used for training and then to save the final trained model for future inference. \n",
        " \n",
        "#### Loss Function and Optimizer\n",
        " - The Loss is defined in the next cell as `loss_fn`.\n",
        " - As defined above, the loss function used will be a combination of Binary Cross Entropy which is implemented as [BCELogits Loss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss) in PyTorch\n",
        " - `Optimizer` is defined in the next cell.\n",
        " - `Optimizer` is used to update the weights of the neural network to improve its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeftvDhjDSPp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4a0a19-8114-451a-98e0-1d89770f6b4c"
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class DistilBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 256)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.classifier = torch.nn.Linear(256, 28)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "model = DistilBERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBERTClass(\n",
              "  (l1): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=256, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=256, out_features=28, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ_wI0YwDVJZ"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO49FuR9DXsW"
      },
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb9-Yr9YDZqo"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if _%5000==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Reta6H84DcJq"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUiMOXPBs6fj"
      },
      "source": [
        "As we can see, the loss rates convert but are still pretty high. This is something we will need to work on during the next weeks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1c-jQ3yq-Ky"
      },
      "source": [
        "<a id='section06'></a>\n",
        "### Validating the Model\n",
        "\n",
        "During the validation stage we pass the unseen data(Testing Dataset) to the model. This step determines how good the model performs on the unseen data. \n",
        "\n",
        "** ANPASSEN **\n",
        "This unseen data is the 20% of `train.csv` which was seperated during the Dataset creation stage. \n",
        "During the validation stage the weights of the model are not updated. Only the final output is compared to the actual value. This comparison is then used to calcuate the accuracy of the model. \n",
        "** ANPASSEN **\n",
        "\n",
        "As defined above to get a measure of our models performance we are using the following metrics. \n",
        "- Hamming Score\n",
        "- Hamming Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV17pa-CZAnJ"
      },
      "source": [
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        acc_list.append(tmp_a)\n",
        "    return np.mean(acc_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UB8UlcerEqg"
      },
      "source": [
        "def validation(testing_loader):\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c48IvQmlrI-S",
        "outputId": "d3333d65-af27-4788-dcce-0c3b7c23e41e"
      },
      "source": [
        "outputs, targets = validation(testing_loader)\n",
        "\n",
        "final_outputs = np.array(outputs) >=0.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  1.32it/s]\u001b[A\n",
            "2it [00:01,  1.33it/s]\u001b[A\n",
            "3it [00:02,  1.34it/s]\u001b[A\n",
            "4it [00:03,  1.02it/s]\u001b[A\n",
            "5it [00:04,  1.10it/s]\u001b[A\n",
            "6it [00:05,  1.17it/s]\u001b[A\n",
            "7it [00:06,  1.09it/s]\u001b[A\n",
            "8it [00:07,  1.15it/s]\u001b[A\n",
            "9it [00:07,  1.18it/s]\u001b[A\n",
            "10it [00:08,  1.36it/s]\u001b[A\n",
            "11it [00:09,  1.36it/s]\u001b[A\n",
            "12it [00:09,  1.35it/s]\u001b[A\n",
            "13it [00:10,  1.37it/s]\u001b[A\n",
            "14it [00:11,  1.39it/s]\u001b[A\n",
            "15it [00:11,  1.35it/s]\u001b[A\n",
            "16it [00:12,  1.36it/s]\u001b[A\n",
            "17it [00:13,  1.37it/s]\u001b[A\n",
            "18it [00:14,  1.33it/s]\u001b[A\n",
            "19it [00:14,  1.35it/s]\u001b[A\n",
            "20it [00:15,  1.51it/s]\u001b[A\n",
            "21it [00:16,  1.47it/s]\u001b[A\n",
            "22it [00:16,  1.44it/s]\u001b[A\n",
            "23it [00:17,  1.44it/s]\u001b[A\n",
            "24it [00:18,  1.47it/s]\u001b[A\n",
            "25it [00:19,  1.04it/s]\u001b[A\n",
            "26it [00:20,  1.15it/s]\u001b[A\n",
            "27it [00:22,  1.13s/it]\u001b[A\n",
            "28it [00:22,  1.01s/it]\u001b[A\n",
            "29it [00:23,  1.12it/s]\u001b[A\n",
            "30it [00:24,  1.25it/s]\u001b[A\n",
            "31it [00:24,  1.27it/s]\u001b[A\n",
            "32it [00:25,  1.38it/s]\u001b[A\n",
            "33it [00:26,  1.25it/s]\u001b[A\n",
            "34it [00:27,  1.28it/s]\u001b[A\n",
            "35it [00:27,  1.29it/s]\u001b[A\n",
            "36it [00:28,  1.32it/s]\u001b[A\n",
            "37it [00:29,  1.29it/s]\u001b[A\n",
            "38it [00:29,  1.44it/s]\u001b[A\n",
            "39it [00:30,  1.41it/s]\u001b[A\n",
            "40it [00:31,  1.39it/s]\u001b[A\n",
            "41it [00:31,  1.29it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwyBV2zCrKrU",
        "outputId": "044014c9-efb0-4af8-f793-2bbd3a68e503"
      },
      "source": [
        "val_hamming_loss = metrics.hamming_loss(targets, final_outputs)\n",
        "val_hamming_score = hamming_score(np.array(targets), np.array(final_outputs))\n",
        "\n",
        "print(f\"Hamming Score = {val_hamming_score}\")\n",
        "print(f\"Hamming Loss = {val_hamming_loss}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hamming Score = 0.08024691358024691\n",
            "Hamming Loss = 0.031715623669646656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pno_zqstPhm"
      },
      "source": [
        "Hamming Loss: 3 % incorrectly predicted labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SweDmI_xrN2g"
      },
      "source": [
        "<a id='section07'></a>\n",
        "### Saving the trained model for inference\n",
        "\n",
        "This is the final step in the process of fine tuning the model. \n",
        "\n",
        "The model and its vocabulary are saved locally. These files are then used to make inferences on new inputs of student research proposals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwnWF9HNrkQy"
      },
      "source": [
        "BA LINH: Ich wei√ü nicht wie man das Vocab speichert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zva4tkgErMve",
        "outputId": "1273b7cf-469c-45d7-d269-880abdf00d3c"
      },
      "source": [
        "# Saving the files for inference\n",
        "\n",
        "#output_model_file = 'pytorch_distilbert_papers.bin'\n",
        "#output_vocab_file = 'vocab_distilbert_papers.bin'\n",
        "\n",
        "path = F\"/content/drive/MyDrive/ThesisAllocationSystem/models/pytorch_distilbert_papers_3.bin\" \n",
        "torch.save(model.state_dict(), path)\n",
        "\n",
        "#path2 = F\"/content/drive/MyDrive/ThesisAllocationSystem/models/pytorch_distilbert_papers.bin\" \n",
        "#tokenizer.save_vocabulary(output_vocab_file.state_dict(), path)\n",
        "\n",
        "#torch.save(model, output_model_file)\n",
        "#tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "print('Saved')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}